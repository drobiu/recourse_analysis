{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5be522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Python-MIP package version 1.12.0 [model.py <module>]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from carla.data.catalog import OnlineCatalog\n",
    "from carla import MLModelCatalog\n",
    "from carla.recourse_methods import Clue, Wachter\n",
    "from carla.models.negative_instances import predict_negative_instances\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from time import perf_counter\n",
    "\n",
    "num = 10\n",
    "data_name = \"compas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3950217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new_model(dataset):\n",
    "    model = MLModelCatalog(dataset, \"ann\", backend=\"pytorch\")\n",
    "    model.train(\n",
    "        learning_rate = 0.001,\n",
    "        epochs = 10,\n",
    "        max_depth = 50,\n",
    "        n_estimators = 50,\n",
    "        batch_size = 20,\n",
    "        force_train = True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74330668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset(dataset, factuals, counterfactuals):\n",
    "    fac_ind = []\n",
    "#     for index, row in factuals.iterrows():\n",
    "#         fac_ind.append(index)\n",
    "#     for index, row in counterfactuals.iterrows():\n",
    "#         dataset.loc[index] = counterfactuals.loc[index]\n",
    "        \n",
    "    for ((i_f, r_f), (i_c, r_c)) in zip(factuals.iterrows(), counterfactuals.iterrows()):\n",
    "        if len(counterfactuals.loc[i_c].dropna()) > 0:\n",
    "            dataset.loc[i_f] = counterfactuals.loc[i_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_recourse_method(method):\n",
    "    rm = None\n",
    "    if method == \"clue\":\n",
    "        hyperparams = {\n",
    "                \"data_name\": data_name,\n",
    "                \"train_vae\": True,\n",
    "                \"width\": 10,\n",
    "                \"depth\": 3,\n",
    "                \"latent_dim\": 12,\n",
    "                \"batch_size\": 64,\n",
    "                \"epochs\": 1,\n",
    "                \"lr\": 0.001,\n",
    "                \"early_stop\": 20,\n",
    "            }\n",
    "\n",
    "        # load a recourse model and pass black box model\n",
    "        rm = Clue(dataset, model, hyperparams)\n",
    "        \n",
    "    else:\n",
    "        hyperparams = {\n",
    "                \"loss_type\": \"BCE\"\n",
    "            }\n",
    "\n",
    "        # load a recourse model and pass black box model\n",
    "        rm = Wachter(model, hyperparams)\n",
    "        \n",
    "        \n",
    "    return rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177339cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    pred = model.predict(data._df)\n",
    "    return np.where(pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_f1_score(model, data):\n",
    "    score = f1_score(np.array(data._df[data.target]), predict(model, data))\n",
    "    print(f\"F1 score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(model, data):\n",
    "    score = accuracy_score(np.array(data._df[data.target]), predict(model, data))\n",
    "    print(f\"Accuracy score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(model_pre, model_post, data):\n",
    "    print(\"Before recourse:\")\n",
    "    print_f1_score(model_pre, data)\n",
    "    print_accuracy(model_pre, data)\n",
    "    print(\"\\nAfter recourse:\")\n",
    "    print_f1_score(model_post, data)\n",
    "    print_accuracy(model_post, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f207857",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16408\\2642490104.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# train a model on the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_new_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# generate counterfactual samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_new_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "dataset = OnlineCatalog(data_name)\n",
    "\n",
    "# train a model on the dataset\n",
    "model = train_new_model(dataset)\n",
    "\n",
    "# generate counterfactual samples\n",
    "factuals = predict_negative_instances(model, dataset._df).sample(num)\n",
    "print(\"Number of factuals\", len(factuals))\n",
    "\n",
    "pre = model.predict(factuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = train_recourse_method(\"wachter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc63e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals = rm.get_counterfactuals(factuals)\n",
    "print(\"Number of counterfactuals:\", len(counterfactuals.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a new dataset\n",
    "d_c = OnlineCatalog(data_name)\n",
    "\n",
    "# replace factuals with counterfactuals\n",
    "update_dataset(d_c._df, factuals, counterfactuals)\n",
    "\n",
    "# train the new model\n",
    "model2 = train_new_model(d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94384fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(model, model2, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(factuals['age'], factuals['length_of_stay'], c=factuals['score'], marker='o')\n",
    "\n",
    "plt.scatter(counterfactuals['age'], counterfactuals['length_of_stay'], c=counterfactuals['score'], marker='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762750ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "factuals.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be9c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27b5d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns.values[0]].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cde490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recourse2",
   "language": "python",
   "name": "recourse2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
