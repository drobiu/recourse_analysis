{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from carla.data.catalog import CsvCatalog\n",
    "from carla import MLModelCatalog\n",
    "from carla.recourse_methods import Clue, Wachter\n",
    "from carla.models.negative_instances import predict_negative_instances\n",
    "from carla.evaluation.benchmark import Benchmark\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "import imageio\n",
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from recourse_util import update_dataset, predict, print_scores \n",
    "\n",
    "num = 10\n",
    "iter_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset):\n",
    "    training_params = {\"lr\": 0.005, \"epochs\": 4, \"batch_size\": 1, \"hidden_size\": [5]}\n",
    "\n",
    "    model = MLModelCatalog(\n",
    "        dataset,\n",
    "        model_type=\"ann\",\n",
    "        load_online=False,\n",
    "        backend=\"pytorch\"\n",
    "    )\n",
    "\n",
    "    model.train(\n",
    "        learning_rate=training_params[\"lr\"],\n",
    "        epochs=training_params[\"epochs\"],\n",
    "        batch_size=training_params[\"batch_size\"],\n",
    "        hidden_size=training_params[\"hidden_size\"],\n",
    "        force_train=True\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69182f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = CsvCatalog(\n",
    "#         file_path='datasets/bimodal_dataset_1.csv',\n",
    "#         file_path='datasets/unimodal_dataset_1.csv',\n",
    "        file_path='datasets/unimodal_dataset_2.csv',\n",
    "        categorical=[],\n",
    "        continuous=['feature1', 'feature2'],\n",
    "        immutables=[],\n",
    "        target='target'\n",
    "    )\n",
    "\n",
    "    data_name = 'custom'\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_recourse_method(method, model, dataset=None, data_name=None, hyperparams=None):\n",
    "    rm = None\n",
    "    if method == \"clue\":\n",
    "        hyperparams = {\n",
    "                \"data_name\": data_name,\n",
    "                \"train_vae\": True,\n",
    "                \"width\": 10,\n",
    "                \"depth\": 3,\n",
    "                \"latent_dim\": 12,\n",
    "                \"batch_size\": 4,\n",
    "                \"epochs\": 5,\n",
    "                \"lr\": 0.0001,\n",
    "                \"early_stop\": 20,\n",
    "            }\n",
    "\n",
    "        # load a recourse model and pass black box model\n",
    "        rm = Clue(dataset, model, hyperparams)\n",
    "        \n",
    "    else:\n",
    "        hyperparams = {\n",
    "            \"loss_type\": \"BCE\",\n",
    "            \"t_max_min\": 0.5/60\n",
    "        }\n",
    "\n",
    "        # load a recourse model and pass black box model\n",
    "        rm = Wachter(model, hyperparams)\n",
    "        \n",
    "        \n",
    "    return rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45839d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(data):\n",
    "\n",
    "    plt.scatter(data['feature1'], data['feature2'], c=data['target'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factuals(dataset, sample_num=5, max_m_iter=3):\n",
    "    m_iter = 0\n",
    "    model = train_model(dataset)\n",
    "    factuals = predict_negative_instances(model, dataset._df)\n",
    "    n_factuals = len(factuals)\n",
    "    while (m_iter < max_m_iter and n_factuals < sample_num):\n",
    "        model = train_model(dataset)\n",
    "        factuals = predict_negative_instances(model, dataset._df)\n",
    "        n_factuals = len(factuals)\n",
    "        m_iter += 1\n",
    "        \n",
    "    return model, factuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103507ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_experiment_iteration(method, dataset, model, factuals, results, draw_state=False):\n",
    "    print(\"Number of factuals\", len(factuals))\n",
    "    \n",
    "#     add_data_statistics(model, dataset, results)\n",
    "    \n",
    "    if method == 'clue':\n",
    "        rm = train_recourse_method('clue', model, dataset, data_name='custom')\n",
    "    else:\n",
    "        rm = train_recourse_method('wachter', model)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    counterfactuals = rm.get_counterfactuals(factuals)\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"Number of counterfactuals:\", len(counterfactuals.dropna()))\n",
    "    \n",
    "    update_dataset(dataset, factuals, counterfactuals)\n",
    "    \n",
    "    benchmark = CustomBenchmark(model, rm, factuals, counterfactuals, start - stop)\n",
    "    results['benchmark'] = benchmark.run_benchmark()\n",
    "    \n",
    "    add_data_statistics(model, dataset, results)\n",
    "    \n",
    "    if draw_state:\n",
    "        draw(dataset._df)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc327b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "def get_empty_results():\n",
    "    return {\n",
    "        'datasets': [],\n",
    "        'means': [],\n",
    "        'covariances': [],\n",
    "        'clustering': [],\n",
    "        'accuracies': [],\n",
    "        'f1_scores': [],\n",
    "        'benchmark': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7464a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_statistics(model, dataset, results):\n",
    "    results['datasets'].append(dataset._df.copy())\n",
    "    results['means'].append(dataset._df[dataset.continuous].mean().to_numpy())\n",
    "    results['covariances'].append(dataset._df[dataset.continuous].cov().to_numpy())\n",
    "    results['clustering'].append(find_elbow(dataset))\n",
    "    results['accuracies'].append(accuracy_score(np.array(dataset._df[dataset.target]), predict(model, dataset)))\n",
    "    results['f1_scores'].append(f1_score(np.array(dataset._df[dataset.target]), predict(model, dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebdd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elbow(dataset, n=10):\n",
    "    ch_metrics = []\n",
    "    x = dataset.df[dataset.continuous]\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        model = KMeans(n_clusters=i, random_state=1).fit(x)\n",
    "        ch_metrics.append(metrics.calinski_harabasz_score(x, model.labels_))\n",
    "        \n",
    "    return ch_metrics.index(np.max(ch_metrics)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609dedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation(results, method='clue'):\n",
    "    data = results[method]['datasets']\n",
    "    names = [f\"images/{method}{str(n)}.png\" for n in range(len(data))]\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        plt.scatter(data[i]['feature1'], data[i]['feature2'], c=data[i]['target'])\n",
    "        plt.savefig(name)\n",
    "        plt.close()\n",
    "        \n",
    "    gif_path = f\"gifs/{method}_gif_{iter_id}.gif\"\n",
    "        \n",
    "    with imageio.get_writer(f'{gif_path}', mode='I') as writer:\n",
    "        for filename in names:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "            \n",
    "    print(f\"Saved gif to {gif_path}\")\n",
    "        \n",
    "    for filename in set(names):\n",
    "        os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13208284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBenchmark(Benchmark):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mlmodel,\n",
    "        recourse_method,\n",
    "        factuals: pd.DataFrame,\n",
    "        counterfactuals: pd.DataFrame,\n",
    "        timer\n",
    "    ) -> None:\n",
    "\n",
    "        self._mlmodel = mlmodel\n",
    "        self._recourse_method = recourse_method\n",
    "        self._counterfactuals = counterfactuals.copy()\n",
    "        self._timer = timer\n",
    "\n",
    "        # Avoid using scaling and normalizing more than once\n",
    "        if isinstance(mlmodel, MLModelCatalog):\n",
    "            self._mlmodel.use_pipeline = False  # type: ignore\n",
    "\n",
    "        self._factuals = factuals.copy()\n",
    "    \n",
    "#     def compute_ynn(self) -> pd.DataFrame:\n",
    "#         return self.super().compute_ynn()\n",
    "    \n",
    "#     def compute_average_time(self) -> pd.DataFrame:\n",
    "#         return self.super().compute_average_time()\n",
    "    \n",
    "#     def compute_distances(self) -> pd.DataFrame:\n",
    "#         return self.super().compute_distances()\n",
    "    \n",
    "#     def compute_constraint_violation(self) -> pd.DataFrame:\n",
    "#         return self.super().compute_constraint_violation()\n",
    "    \n",
    "#     def compute_redundancy(self) -> pd.DataFrame:\n",
    "#         return self.super().compute_redundancy()\n",
    "    \n",
    "#     def compute_success_rate(self) -> pd.DataFrame:\n",
    "#         return self.super().compute_success_rate()\n",
    "    \n",
    "#     def run_benchmark(self) -> pd.DataFrame:\n",
    "#         return self.super().run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_id += 1\n",
    "dataset = load_dataset()\n",
    "\n",
    "clue_dataset = load_dataset()\n",
    "clue_result = get_empty_results()\n",
    "results['clue'] = clue_result\n",
    "\n",
    "wachter_dataset = load_dataset()\n",
    "wachter_result = get_empty_results()\n",
    "results['wachter'] = wachter_result\n",
    "\n",
    "iterations = 10\n",
    "samples = 5\n",
    "\n",
    "for i in range(iterations):\n",
    "    clue_model, clue_factuals = get_factuals(clue_dataset, sample_num=samples)\n",
    "    wachter_model, wachter_factuals = get_factuals(wachter_dataset, sample_num=samples)\n",
    "    \n",
    "    factuals = pd.merge(clue_factuals, wachter_factuals, how='inner', on=[*dataset.continuous, dataset.target])\n",
    "    factuals = pd.merge(factuals, dataset._df, how='inner', on=dataset.continuous)\n",
    "    \n",
    "    if len(factuals) > samples:\n",
    "        factuals = factuals.sample(samples)\n",
    "    \n",
    "    execute_experiment_iteration('clue', clue_dataset, clue_model, factuals, clue_result)\n",
    "    execute_experiment_iteration('wachter', wachter_dataset, wachter_model, factuals, wachter_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79765a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_animation(results, 'clue')\n",
    "generate_animation(results, 'wachter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3226a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recourse2",
   "language": "python",
   "name": "recourse2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
